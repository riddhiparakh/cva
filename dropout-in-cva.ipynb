{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"Architecture building questions\n\n- trainable params, non trainable params and total params count\n- pseudo code\nE.g. \n\nInput_layer\nConv(32, kernels=3, strides=2, padding=same)\n.\n.\n.\nDense(100, softmax)\n\nArchitecture drawing question if given as a follow up question \n\n- illustration of the architecture\n- explanation of module blocks like inception module, resnet block, mobilnet block if used\n\nManual Parameter calculation\n- sum to be solved on the paper using tables\n- necessary formulas to be mentioned just once\n\nmAP\n- sum to be solved using the table and the precision and recall curve needs to be shown\n- final answer should be in %, should be approx. close\n\nModel Evaluation\n- to be solved on the paper using tables\n- calculation can be done on excel, but formula of calculation needs to be shown in the paper\n\nProgramming questions:-\n\nClassification\n- loss curve illustration on paper\n- fitting of the model\n- classification report \n- confusion matrix\n- suggestions for how model can be improved\n- auc roc curve (in case of binary classification)\n\nTransformations and SIFT\n- same as M1\n\nVisual Similarity search using Image embeddings\n- product similarity that was covered in the class, algorithm of how similar products are recommended needs to be explained on paper \n- observation needs to be written e.g. if all k products are matching with the input product based on what similarities e.g color, size, etc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weight shearing","metadata":{}},{"cell_type":"markdown","source":"Weight sharing is a technique used in Convolutional Neural Networks (CNNs) to reduce the number of trainable parameters and improve the model's efficiency.\n\nIn a CNN, the convolutional layer is responsible for extracting features from the input image. Each filter in the convolutional layer convolves over the entire input image, generating a feature map. Traditionally, each filter has its own set of learnable parameters, which can quickly add up to a large number of parameters. However, weight sharing allows the filters to share the same set of learnable parameters.\n\nBy sharing the weights, the same feature detector is applied to multiple regions of the input image. This reduces the number of parameters that need to be learned and can improve the model's generalization ability, as it forces the network to learn features that are invariant to translation.\n\nFor example, consider a 3x3 filter in the first convolutional layer of a CNN. If we have a 32x32 input image, there are 30x30 (900) possible locations where the filter can be applied. If each filter had its own set of learnable parameters, we would have to learn 900 sets of parameters just for the first layer. However, by sharing the weights, we only need to learn one set of parameters for the filter, regardless of how many times it is applied to the input image.\n\nIn summary, weight sharing in CNNs can reduce the number of parameters, improve the model's efficiency, and force the network to learn features that are invariant to translation.","metadata":{}},{"cell_type":"markdown","source":"# Rotation, scale, and translational invariance\nRotation, scale, and translational invariance are important properties that are often desired in image processing and computer vision tasks. Here is a brief overview of each of these properties:\n\nRotation invariance: A system is said to be rotation invariant if it can recognize an object in an image, regardless of the object's orientation. In other words, if the object is rotated by some angle, the system should still be able to recognize it. This property is important in tasks such as object recognition and face detection, where the objects of interest can be oriented in different ways.\n\nScale invariance: A system is said to be scale invariant if it can recognize an object in an image, regardless of the object's size. In other words, if the object is scaled up or down, the system should still be able to recognize it. This property is important in tasks such as object recognition and scene understanding, where objects can appear at different scales.\n\nTranslational invariance: A system is said to be translational invariant if it can recognize an object in an image, regardless of the object's position. In other words, if the object is shifted to a different location in the image, the system should still be able to recognize it. This property is important in tasks such as object detection and tracking, where objects can appear at different locations in the image.\n\nThere are various techniques used to achieve these properties in image processing and computer vision tasks. For example, in Convolutional Neural Networks (CNNs), weight sharing is used to achieve translational invariance, while pooling layers can be used to achieve scale invariance. In addition, data augmentation techniques such as rotating, scaling, and shifting the images can also help achieve these properties.","metadata":{}},{"cell_type":"markdown","source":"# Dropout\n\nDropout is a regularization technique used in neural networks to prevent overfitting. It works by randomly setting a fraction of the input units of a layer to zero during training. This means that these units are ignored and do not contribute to the forward or backward pass of the network during that particular training iteration.\nThe dropout technique has been shown to be effective in reducing overfitting, improving generalization performance and making the neural network more robust. By randomly dropping out some of the neurons, the network is forced to learn more robust features that are not dependent on the presence of specific neurons. This can prevent overfitting, as the network cannot rely on a specific set of neurons to make accurate predictions.\nDuring inference (testing), the dropout layer is turned off, and all units are used, as the goal is to get accurate predictions on new unseen data.\nIn summary, dropout is a regularization technique used in neural networks to prevent overfitting. It works by randomly dropping out some of the neurons during training, which can force the network to learn more robust features and prevent it from relying on a specific set of neurons.\n\n","metadata":{}},{"cell_type":"markdown","source":"# transfer-learning","metadata":{}},{"cell_type":"markdown","source":"Transfer learning is a machine learning technique in which a pre-trained model is used as a starting point for a new task. Instead of training a new model from scratch, the pre-trained model's knowledge is transferred to the new task by fine-tuning the pre-trained model or using it as a fixed feature extractor.\n\nIn transfer learning, the pre-trained model has already learned features from a large amount of data, typically from a different but related task. By reusing these learned features, the model can save time and computational resources in the training process, and often achieve better performance on the new task with less data.\n\nThere are two main approaches to transfer learning:\n\nFine-tuning: In this approach, the pre-trained model's weights are used as the initial weights for a new model, which is then trained on the new task using the new dataset. The weights of the pre-trained model are then adjusted (fine-tuned) during training to adapt to the new task. Fine-tuning is commonly used in computer vision tasks, such as object detection and classification, where pre-trained models like VGG, ResNet, and Inception have been used as starting points.\n\nFeature extraction: In this approach, the pre-trained model is used as a fixed feature extractor, where the pre-trained model's weights are frozen, and the input data is fed through the pre-trained model to extract features. The extracted features are then used as inputs to a new model, which is trained on the new task. Feature extraction is commonly used in natural language processing (NLP) tasks, such as sentiment analysis and text classification, where pre-trained models like BERT and GPT have been used as starting points.\n\nIn summary, transfer learning is a powerful technique that allows us to leverage pre-trained models to achieve better performance on new tasks with less data and computational resources. It has been widely used in computer vision and natural language processing tasks, and it continues to be an active area of research.\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Visual embeddings\n\nVisual embeddings are similar to word embeddings, but they represent images or visual features as vectors of numerical values. They are used in computer vision tasks such as image classification, object detection, and image captioning.\n\nThere are several methods for learning visual embeddings, including supervised and unsupervised techniques. One common method is to use a pre-trained convolutional neural network (CNN) such as VGG, ResNet, or Inception, which has been trained on large datasets like ImageNet. The activations from the last fully connected layer or the penultimate layer (before the final classification layer) can be used as a visual embedding for the input image.","metadata":{}},{"cell_type":"markdown","source":"# Siamese network\n\nA Siamese network is a type of neural network architecture that is designed for learning similarity or distance between two inputs. It is called a Siamese network because it consists of two identical subnetworks that share the same weights and architecture, just like conjoined twins.\n\nThe two inputs to a Siamese network can be anything, such as two images, two sentences, or two audio signals. The network learns to output a similarity score between the two inputs, indicating how similar or dissimilar they are.\n\nThe architecture of a Siamese network typically consists of several convolutional or recurrent layers followed by one or more fully connected layers. The two inputs are passed through the two subnetworks, and their outputs are concatenated and passed through the fully connected layers to output a similarity score.\n\nDuring training, the network is typically trained with pairs of inputs and their corresponding similarity labels. The loss function used for training is usually a contrastive loss or triplet loss, which encourages the network to learn embeddings for the two inputs that are close together in the embedding space if they are similar and far apart if they are dissimilar.\n\nSiamese networks have been used in various applications, including face recognition, signature verification, image retrieval, and natural language processing tasks such as sentence similarity and paraphrase detection. The use of Siamese networks has also been extended to multi-input scenarios, where the network learns similarity or distance between multiple inputs.\n\nOverall, Siamese networks are a powerful architecture for learning similarity or distance between inputs, and they have been successfully applied in various domains.","metadata":{}},{"cell_type":"markdown","source":"# Triple loss\nTriple loss is a loss function commonly used in Siamese networks and other deep learning models for learning similarity or distance between inputs. It is called \"triple\" loss because it is computed based on triplets of examples.\n\nThe idea behind triple loss is to learn embeddings of inputs such that the distance between similar inputs is small and the distance between dissimilar inputs is large. To achieve this, the network is trained with triplets of examples, where each triplet consists of an anchor example, a positive example (similar to the anchor), and a negative example (dissimilar to the anchor).\n\nThe triple loss function is defined as:\n\nL = max(0, d(a, p) - d(a, n) + margin)\n\nwhere:\n\nd(a, p) is the distance between the anchor and positive examples in the embedding space\nd(a, n) is the distance between the anchor and negative examples in the embedding space\nmargin is a hyperparameter that controls the minimum margin between the distances of the anchor-positive and anchor-negative pairs\nThe loss is computed for each triplet, and the goal is to minimize the loss over all triplets in the training set.\n\nThe triple loss encourages the network to learn embeddings such that the distance between the anchor and positive examples is smaller than the distance between the anchor and negative examples by at least the margin value. This results in embeddings that are clustered together for similar examples and separated for dissimilar examples.\n\nTriple loss has been used in various applications, such as face recognition, image retrieval, and natural language processing tasks such as sentence similarity and paraphrase detection.","metadata":{}},{"cell_type":"markdown","source":"# roc-auc curve\nAn ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n•True Positive Rate •False Positive Rate\n\nROC stands for Receiver Operating Characteristic, and it is a graphical plot that illustrates the performance of a binary classification model at different classification thresholds.\n\nIn an ROC curve, the x-axis represents the False Positive Rate (FPR), which is the proportion of negative examples that are incorrectly classified as positive, while the y-axis represents the True Positive Rate (TPR), which is the proportion of positive examples that are correctly classified as positive.\n\nThe ROC curve is created by varying the classification threshold of the model and plotting the TPR against the FPR for each threshold. The resulting curve shows how well the model is able to distinguish between the positive and negative classes as the classification threshold is varied.\n\nA good classifier will have a ROC curve that hugs the top-left corner of the plot, indicating a high TPR and a low FPR at all classification thresholds. A random classifier will have a ROC curve that is a straight line from the bottom-left to the top-right corner, with an AUC of 0.5.\n\n\n\nAUC stands for Area Under the Curve, and it is a common metric used to evaluate the performance of binary classification models. The AUC is a measure of the overall ability of the model to distinguish between the positive and negative classes, regardless of the classification threshold.\n\nThe AUC metric is calculated by plotting the Receiver Operating Characteristic (ROC) curve of the model. The ROC curve is a plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) for different classification thresholds. The TPR is the proportion of positive examples that are correctly classified as positive, while the FPR is the proportion of negative examples that are incorrectly classified as positive.\n\nThe AUC is then calculated as the area under the ROC curve. The AUC ranges from 0 to 1, with a value of 0.5 indicating a random classifier and a value of 1 indicating a perfect classifier. A higher AUC value indicates a better performance of the model in distinguishing between the positive and negative classes.\n\nThe AUC metric is commonly used in machine learning applications, such as medical diagnosis, fraud detection, and spam filtering, where the cost of false positives and false negatives may be different. The AUC metric provides a comprehensive measure of the model's performance, taking into account both sensitivity and specificity, and is widely used in practice.","metadata":{}},{"cell_type":"markdown","source":"# Why should one monitor validation loss while saving the best model instead ofaccuracy ?\n\nWhen training a machine learning model, it is important to monitor its performance on a validation set, which is a set of examples that the model has not seen during training. The validation set is used to evaluate the generalization performance of the model and to prevent overfitting.\n\nWhile accuracy is a commonly used metric to evaluate the performance of a classification model, it may not always be the best metric to use for model selection. This is because accuracy can be misleading, especially when the classes are imbalanced. For example, if the positive class is rare, a model that always predicts the negative class will have a high accuracy but will not be useful in practice.\n\nOn the other hand, validation loss is a more reliable metric to use for model selection. Validation loss measures the difference between the predicted output and the actual output, averaged over the validation set. By minimizing the validation loss, the model is optimized to make accurate predictions on new, unseen data. Additionally, validation loss is more sensitive to changes in the model parameters than accuracy, making it a better indicator of overfitting.\n\nTherefore, it is recommended to monitor the validation loss while training a machine learning model and to save the model that achieves the lowest validation loss, rather than the model that achieves the highest accuracy. This approach will help to ensure that the model is generalizable and performs well on new data.\n","metadata":{}},{"cell_type":"markdown","source":"# yolo v1\nIn Region based cnns we have 3 and 2 stage pipeline to detect an object.\nYOLO v1 uses a single convolutional neural network to perform both object localization and classification. Yolo stands for you only look once.\nwe look at the image only once. \nYOLO v1 works by dividing an input image into a grid of cells and if center of an object falls into a grid cell that cell is reponsible for detecting object.\n\nFor each grid cell, YOLO predicts a total of B bounding boxes (with B=2 as described in the paper) and corresponding confidence scores for those boxes. These confidence scores indicate the likelihood of the box containing an object, which is essentially the probability of the box prediction.grid cell also predicts c conditional class probabilties where c is the number of class\n\nEach bounding box is defined by five predictions, namely, x, y, w, h, and confidence. The (x, y) coordinates denote the center of the box relative to the boundaries of the grid cell. On the other hand, the width (w) and height (h) of the box are predicted relative to the whole image, not just the grid cell.\n\nFurthermore, the confidence score represents the intersection over union (IOU) between the predicted box and any actual or ground truth box in the training data. In other words, it reflects how well the predicted box aligns with the ground truth box.\n\nsxsx(bx5+c)\n\nThe ground truth data for object detection consisting of bounding box coordinates and class predictions should be structured in the shape of the above output\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# convolution \nIn a CNN, convolution involves passing a filter or kernel over the input data, such as an image, and computing the dot product between the filter weights and the corresponding pixel values in the input. This process generates a feature map or output, which represents the filtered or convolved features of the input.\n\nConvolution is effective in detecting local patterns or features in the input data, such as edges, corners, and textures, which are essential for recognizing objects in images or other types of data. Additionally, convolutional layers can learn and extract high-level features that are more complex and abstract, by combining and transforming the lower-level features extracted from earlier layers in the network.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}